#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import sys, os
import argparse, json, fileinput
import math, random
import numpy as np

import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from sklearn import cluster
from sklearn import mixture
import pickle

from scipy import sparse


estimators_help = {
            'AffinityPropagation': 'Perform Affinity Propagation Clustering of data.',
            'AgglomerativeClustering': 'Agglomerative Clustering',
            'Birch': 'Implements the Birch clustering algorithm.',
            'DBSCAN': 'Perform DBSCAN clustering from vector array or distance matrix.',
            'FeatureAgglomeration': 'Agglomerate features.',
            'KMeans': 'K-Means clustering',
            'MiniBatchKMeans': 'Mini-Batch K-Means clustering',
            'MeanShift': 'Mean shift clustering using a flat kernel.',
            'SpectralClustering': 'Apply clustering to a projection to the normalized laplacian.',
            'GaussianMixture': 'Gaussian Mixture Model',
            'BayesianGaussianMixture': 'Variational Bayesian estimation of a Gaussian mixture.',
            } # estimator help text, estimator objects constructed from the keys

estimators_params = dict() # estimator parameters
estimators = dict() # estimator classes

# fill estimator dicts
def get_estimators():
    for e in list(estimators_help.keys()):
        tmp = None
        module = None

        if e == 'GaussianMixture' or e == 'BayesianGaussianMixture':
            module = mixture
        else:
            module = cluster

        try:
            tmp = getattr(module, e)
        except AttributeError:
            #raise NotImplementedError("Class `{}` does not implement `{}`".format(module.__name__, e))
            sys.stderr.write("train-skl: " + module.__name__ + ' does not implement estimator ' + e + '\n')
            sys.exit(-1)

        estimators[e] = tmp()
        estimators_params[e] = estimators[e].get_params()

    #print_dict('estimators_help', estimators_help, '')
    #print_dict('estimators_params', estimators_params, '')
    #print_dict('estimators', estimators, '')


# load stream
def load(stream, trainset):
    train_labels = list()
    train_features_l = list()
    test_labels = list()
    test_features_l = list()
    num_features = 0

    ratio, fraction, integral = 0, 0, 0

    # get trainset modes
    # TODO: support k greater 9
    try:
        ratio = float(trainset)
        fraction, integral = math.modf(ratio)
        integral = int(integral)
        fraction = round(fraction*10)
    except ValueError:
        stream = trainset

    # special case for a ratio of 100%
    if ratio == 1:
        ratio = 0

    # sanity checks on trainset
    if ratio > 0:
        if fraction < 0:
            sys.stderr.write("train-skl: no fold number given, specify with k.x or use a ratio (0,1] for random split\n")
            sys.exit(-1)

        if integral >= 1 and fraction > integral:
            sys.stderr.write("train-skl: fold number (" + str(fraction) + ") must be greater than 0 and less than or equal the number of folds (" + str(integral) + ")\n")
            sys.exit(-1)

        if ratio > 1 and fraction < 1:
            sys.stderr.write("train-skl: fold number (" + str(fraction) + ") must be greater than 0 and less than or equal the number of folds (" + str(integral) + ")\n")
            sys.exit(-1)

        if integral > 9:
            sys.stderr.write("train-skl: k > 9 not supported currently\n")
            sys.exit(-1)

    for line in fileinput.input(stream, bufsize=1000):
        line = line.strip()
        line = line.strip('\n')

        # skip empty and comment lines
        if line == "":
            continue
        if line[0] == '#':
            #print(line)
            continue

        fields = line.split()
        if len(fields) < 2:
            sys.stderr.write("train-skl: no features?\n")
            sys.exit(-1)

        if not num_features:
            num_features = len(fields[1:])
        elif len(fields[1:]) != num_features:
            sys.stderr.write("train-skl: incorrect number of features:" + str(len(fields[1:])) + "!=" + str(num_features) + '\n')
            sys.exit(-1)

        train_labels.append(fields[0])
        train_features_l.append([ float(x) for x in fields[1:] ])

    # do splitting
    if ratio <= 0:
        pass
    elif ratio < 1:
        # random stratified split:
        # 1. for each class, create a vector containing their respective indices (strata)
        # 2. randomize each stratum and split them according to the provided ratio
        # 3. according to the created index strata, move the selected samples to the test vectors

        # init
        train_strata = list()
        test_strata = list()
        u_labels = list(set(train_labels))

        # create strata vector with sample indices
        for l in u_labels:
            train_strata.append([ x for x,y in enumerate(train_labels) if y==l ])

        # randomize index strata
        for s in train_strata:
            random.shuffle(s)

        # split index strata
        for s in train_strata:
            test_strata.append([ y for x,y in enumerate(s) if x < len(s) * (1-ratio) ])

        # combine test strata to one list and sort it
        test_set = list()
        for s in test_strata:
            test_set.extend(s)
        test_set.sort(reverse=True)

        # move test set samples from train list to test list
        for i in test_set:
            test_labels.insert(0, train_labels.pop(i))
            test_features_l.insert(0, train_features_l.pop(i))

    elif ratio > 1:
        # k-fold split:
        samples_per_fold = int(len(train_labels) / integral)

        # last fold might be larger since only integers
        if integral == fraction:
            for i in reversed(range((fraction-1) * samples_per_fold, len(train_labels))):
                test_labels.insert(0, train_labels.pop(i))
                test_features_l.insert(0, train_features_l.pop(i))
        else:
            for i in reversed(range(0, samples_per_fold)):
                test_labels.insert(0, train_labels.pop((fraction - 1) * samples_per_fold + i))
                test_features_l.insert(0, train_features_l.pop((fraction - 1) * samples_per_fold + i))

    return train_labels, np.array(train_features_l), test_labels, np.array(test_features_l)



# output graphs showing the results of the last fitting of the given estimator
def graph_result(estimator, labels, features):
    ulabels, ilabels = list(), list()
    for i in labels:
        if ulabels.count(i) == 0:
            ulabels.append(i)
    for i in labels:
        ilabels.append(ulabels.index(i))

    print("samples:", len(labels))
    print("uniques:", len(ulabels))
    print("features:", len(features[0]))
    print("estimator:", estimator)

    try:
        centers = estimator.cluster_centers_
    except AttributeError:
        sys.stderr.write("train-skl: sorry, graphing for the selected estimator is not supported yet\n")
        sys.exit(-1)

    fig = plt.figure(1, figsize=(20,12))
    fig.clf()
    fig.canvas.set_window_title('train-skl: ' + str(estimator)[:str(estimator).find("(")])

    ax = fig.add_subplot(2,3,1)
    ax.scatter(features[:,0], features[:,1], c=ilabels, cmap=plt.get_cmap('gist_rainbow'))
    ax.scatter(centers[:,0], centers[:,1], marker='x', color='r', s=150, linewidths=2)
    ax.set_title('X-Y')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    ax = fig.add_subplot(2,3,2)
    ax.scatter(features[:,0], features[:,2], c=ilabels, cmap=plt.get_cmap('gist_rainbow'))
    ax.scatter(centers[:,0], centers[:,2], marker='x', color='r', s=150, linewidths=2)
    ax.set_title('X-Z')
    ax.set_xlabel('X')
    ax.set_ylabel('Z')

    ax = fig.add_subplot(2,3,3)
    ax.scatter(features[:,1], features[:,2], c=ilabels, cmap=plt.get_cmap('gist_rainbow'))
    ax.scatter(centers[:,1], centers[:,2], marker='x', color='r', s=150, linewidths=2)
    ax.set_title('Y-Z')
    ax.set_xlabel('Y')
    ax.set_ylabel('Z')

    ax = fig.add_subplot(2,3,5, projection='3d')
    ax.scatter(features[:,0], features[:,1], features[:,2], c=ilabels, cmap=plt.get_cmap('gist_rainbow'))
    ax.scatter(centers[:,0], centers[:,1], centers[:,2], marker='x', color='r', s=150, linewidths=2)
    ax.set_title('X-Y-Z')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')

    plt.show()



# helper to print dictionaries somewhat readable
def print_dict(header, d, *args):
    pad_text = len(max(list(d.keys()), key=len)) # longest string
    print(header)
    for i in sorted(d.keys()):
        print("  ", '{:<{}}'.format(i, pad_text), "   ", d[i], sep='')
    for i in args:
        print(i)

# helper to make a custom args compatibility string into a dictionary-able string
def custom_args_compat(s):
    # check if even number of entries
    if len(s.split(':')) % 2 != 0:
        sys.stderr.write("Number of dictionary entries must be even!\n")
        sys.exit(-1)

    # first and last double quote
    s = s[0] + '"' + s[1:-1] + '"' + s[-1]

    # insert double quote before and after each colon
    i = 0
    while True:
        if i >= len(s): break
        if s[i] == ':':
            s = s[:i] + '":"' + s[i+1:]
            i += 2
        else: i += 1

    # replace every second colon with comma
    flag = False
    for i in range(len(s)):
        if s[i] == ':' and flag:
            s = s[:i] + ',' + s[i+1:]
            flag = False
        elif s[i] == ':' and not flag: flag = True
    return s



def general_args(parser):
    parser.add_argument('samples', metavar='SAMPLES', type=str, nargs='?', default='-', help="sample stream, format: [label] [[features]]\n")
    parser.add_argument('-g', '--graph', help='graph results of estimator fitting\n', action='store_true')
    parser.add_argument('-p', '--prediction', help='output the prediction for the test set, or if no test set specified, the train set\n', action='store_true')
    parser.add_argument('-f', '--file', type=argparse.FileType('wb'), help='save the estimator model to the file\n')
    parser.add_argument('-n', '--trainset', type=str, default='0', help='training/test set customization:\n- <= 0: no split\n- < 1: random stratified split, rest is test set\n- > 1: k-fold split, selected fold is test set (k.x, [folds].[select])\n- file: use this file as sample source, no split\n')
    parser.add_argument('--custom-args', metavar='dict', type=str, help='custom trainer arguments\n')
    parser.add_argument('--custom-args-compat', help='compatibility mode for --custom-args, e.g. when used with GNU parallel\nkeys and values are strings only! format: {key1:value1:key2...}\n', action='store_true')

if __name__=="__main__":
    get_estimators()

    class Formatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter): pass
    cmdline = argparse.ArgumentParser(description="Unsupervised clustering algorithms from scikit-learn\n[train module]", epilog="Default output: estimator dump", formatter_class=Formatter)

    # create subparsers for each estimator
    est_cmdline = cmdline.add_subparsers(title='estimator', dest='estimator', description='used to fit model to sample features\nSee http://scikit-learn.org/stable/modules/clustering.html for more info.', metavar='ESTIMATOR')
    est_cmdline_subs = dict()
    for e in sorted(estimators_help.keys()):
        est_cmdline_subs[e] = est_cmdline.add_parser(e, help=estimators_help[e], description=estimators_help[e]+'\n\nSee http://scikit-learn.org/stable/modules/clustering.html for more info.', formatter_class=Formatter)
        general_args(est_cmdline_subs[e])
        # fill arguments with available parameters
        group = est_cmdline_subs[e].add_argument_group('estimator parameters')
        for a in sorted(estimators_params[e].keys()):
            group.add_argument('--'+a, type=type(estimators_params[e][a]), default=estimators_params[e][a], metavar=type(estimators_params[e][a]).__name__, help=' ')

    args = cmdline.parse_args()

    if args.estimator == None:
        print('no estimator given')
        cmdline.print_help()
        quit()

    # sklearn now does this by itself
    #if args.estimator in {'AffinityPropagation', 'SpectralClustering'}:
    #    print('# NOTE: Selected algorithm takes a similarity matrix as input!')
    #    # TODO: can we automatically transform samples into similarity matrix in this case?

    # load sample stream and fill label and feature containers
    train_labels, train_features, test_labels, test_features = load(args.samples, args.trainset)

    # get the estimator class
    estimator = estimators[args.estimator]

    # set the estimators parameters from cli
    params = dict(args.__dict__)
    # filter other cli arguments
    del params['estimator'], params['samples'], \
        params['graph'], params['prediction'], \
        params['file'], params['trainset'], \
        params['custom_args'], params['custom_args_compat']
    if args.custom_args_compat:
        args.custom_args = custom_args_compat(args.custom_args)
    custom_args = json.loads(args.custom_args)
    if 'connectivity' in custom_args and custom_args['connectivity'] == 'temp_seq':
        temp_seq = sparse.diags([1,1], [-1,1], shape=(len(train_features), len(train_features)))
        params['connectivity'] = temp_seq
    estimator.set_params(**params)

    #print(estimator.get_params())

    # fit model
    try:
        estimator.fit(train_features)
    except ValueError as ex:
        sys.stderr.write("train-skl: ValueError when fitting estimator!\n")
        sys.stderr.write(str(ex) + '\n')
        sys.exit(-1)

    # graph
    if args.graph:
        graph_result(estimator, train_labels, train_features)
        quit()

    if args.prediction:
        if len(test_labels) == 0:
            try:
                pred = estimator.fit_predict(train_features)
            except AttributeError:
                pred = estimator.predict(train_features)
            for i in range(0, len(train_labels)): print(train_labels[i], pred[i], sep='\t')
        else:
            pred = estimator.predict(test_features)
            for i in range(0, len(test_labels)): print(test_labels[i], pred[i], sep='\t')
        quit()

    if args.file:
        pickle.dump(estimator, args.file)
    else:
        print(pickle.dumps(estimator).__sizeof__()) # size of the object, to be read by predict TODO: 33 Bytes inaccurate
        print(pickle.dumps(estimator).decode('latin-1'), flush=True)

    if len(test_labels) > 0:
        print()
        print()
        for i in range(0, len(test_labels)):
            print(test_labels[i], end='\t')
            for feature in test_features[i]:
                print(feature, end='\t')
            print()



